{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff2e876-20b7-4e75-9fbe-dae3c6094dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input model (FP32) from absolute path: /root/zscloud-tmp/demo/models/bert_pruned_16_layers_finetuned/best_model\n",
      "Output state_dict file for INT8 model: ./models/bert_pruned_16L_int8_ptq.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_822/1125519941.py:36: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_int8 = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT-Large model successfully quantized to INT8.\n",
      "\n",
      "--- INT8 Model State Dict for 16L Bert-Large Saved to: ./models/bert_pruned_16L_int8_ptq.pt ---\n"
     ]
    }
   ],
   "source": [
    "# quantize_ptq_int8_large.py\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import os # <-- 1. Import the 'os' library\n",
    "\n",
    "# --- 1. 配置 ---\n",
    "# --- 2. Correct the path by converting it to an absolute path ---\n",
    "# Define the relative path first\n",
    "RELATIVE_FP32_MODEL_PATH = \"./models/bert_pruned_16_layers_finetuned/best_model\"\n",
    "# Convert it to an absolute path\n",
    "PRUNED_FP32_MODEL_PATH = os.path.abspath(RELATIVE_FP32_MODEL_PATH)\n",
    "\n",
    "# Check if the path exists to avoid confusion\n",
    "if not os.path.isdir(PRUNED_FP32_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"The model directory does not exist at the specified path: {PRUNED_FP32_MODEL_PATH}\")\n",
    "\n",
    "# Output file for the INT8 quantized model weights\n",
    "QUANTIZED_INT8_SAVE_PATH = \"./models/bert_pruned_16L_int8_ptq.pt\"\n",
    "\n",
    "print(f\"Input model (FP32) from absolute path: {PRUNED_FP32_MODEL_PATH}\")\n",
    "print(f\"Output state_dict file for INT8 model: {QUANTIZED_INT8_SAVE_PATH}\")\n",
    "\n",
    "# --- 2. 加载 FP32 模型 ---\n",
    "# Load from the CPU, as quantization is for CPU deployment\n",
    "# The from_pretrained function will now correctly interpret the absolute path\n",
    "model_fp32 = AutoModelForSequenceClassification.from_pretrained(PRUNED_FP32_MODEL_PATH)\n",
    "model_fp32.to(\"cpu\")\n",
    "model_fp32.eval()\n",
    "\n",
    "# --- 3. 应用动态量化 ---\n",
    "# Import PyTorch's quantization module\n",
    "import torch.quantization\n",
    "\n",
    "# Apply dynamic quantization to all Linear layers in the model\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,\n",
    "    {torch.nn.Linear}, # Specify the layer types to quantize\n",
    "    dtype=torch.qint8  # Specify the target data type as int8\n",
    ")\n",
    "\n",
    "print(\"\\nBERT-Large model successfully quantized to INT8.\")\n",
    "\n",
    "# --- 4. 保存量化后的模型权重 ---\n",
    "# Ensure the parent directory for the output file exists\n",
    "os.makedirs(os.path.dirname(QUANTIZED_INT8_SAVE_PATH), exist_ok=True)\n",
    "\n",
    "# Save only the model's state_dict (weights and parameters)\n",
    "torch.save(model_int8.state_dict(), QUANTIZED_INT8_SAVE_PATH)\n",
    "\n",
    "print(f\"\\n--- INT8 Model State Dict for 16L Bert-Large Saved to: {QUANTIZED_INT8_SAVE_PATH} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575e253-a6ca-4b27-999a-9900b3895722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
