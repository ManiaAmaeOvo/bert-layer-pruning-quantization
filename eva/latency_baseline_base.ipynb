{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325a2d44-a8d9-4c73-aabc-5b2b52bc2b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from: bert-base-uncased\n",
      "Loading model from: ./my_bert_sst2_finetuned/checkpoint-1800\n",
      "Running 10 warmup runs...\n",
      "Running 100 measurement runs...\n",
      "------------------------------\n",
      "Inference Latency on cuda:\n",
      "  Average: 2.76 ms\n",
      "  Std Dev: 0.08 ms\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# --- 配置 ---\n",
    "\n",
    "# !! 关键修改：将模型路径和分词器标识符分开\n",
    "MODEL_PATH = \"./my_bert_sst2_finetuned/checkpoint-1800\"\n",
    "TOKENIZER_NAME = \"bert-base-uncased\"  # <--- 使用你微调时用的原始分词器名称\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TEST_SENTENCE = \"This is a great movie, I really enjoyed it.\"\n",
    "WARMUP_RUNS = 10\n",
    "MEASURE_RUNS = 100\n",
    "\n",
    "# --- 1. 加载模型和分词器 ---\n",
    "\n",
    "print(f\"Loading tokenizer from: {TOKENIZER_NAME}\")\n",
    "# 从原始预训练模型的名称加载分词器，它会从Hugging Face Hub或本地缓存下载\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "# 从你保存的、包含微调后权重的路径加载模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# --- 2. 准备输入 ---\n",
    "inputs = tokenizer(TEST_SENTENCE, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# --- 3. 预热 (Warmup) ---\n",
    "print(f\"Running {WARMUP_RUNS} warmup runs...\")\n",
    "with torch.no_grad():\n",
    "    for _ in range(WARMUP_RUNS):\n",
    "        _ = model(**inputs)\n",
    "\n",
    "# --- 4. 测量 ---\n",
    "print(f\"Running {MEASURE_RUNS} measurement runs...\")\n",
    "timings = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(MEASURE_RUNS):\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        _ = model(**inputs)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        timings.append(end_time - start_time)\n",
    "\n",
    "# --- 5. 计算并报告结果 ---\n",
    "avg_latency_ms = np.mean(timings) * 1000\n",
    "std_latency_ms = np.std(timings) * 1000\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Inference Latency on {DEVICE}:\")\n",
    "print(f\"  Average: {avg_latency_ms:.2f} ms\")\n",
    "print(f\"  Std Dev: {std_latency_ms:.2f} ms\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2309fa-529c-41cb-9c80-cd3525c0f017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
