{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da1e689e-9674-45b2-a55a-702f71528353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input model (FP32): ./models/bert_pruned_8_layers_finetuned/best_model\n",
      "Output state_dict file for INT8 model: ./models/bert_pruned_8L_int8_ptq.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1515/4125737800.py:27: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_int8 = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model successfully quantized to INT8.\n",
      "Original FP32 model sample (first layer weight):\n",
      "Parameter containing:\n",
      "tensor([[-0.0173,  0.0278, -0.0232,  ...,  0.0160,  0.0754,  0.0555],\n",
      "        [-0.0367,  0.0362, -0.0440,  ..., -0.0525,  0.1387,  0.0082],\n",
      "        [ 0.0123,  0.0343,  0.0110,  ..., -0.0253,  0.0251, -0.0463],\n",
      "        ...,\n",
      "        [-0.0071,  0.0507,  0.0551,  ...,  0.0298,  0.0533, -0.0514],\n",
      "        [-0.0174,  0.0945,  0.0594,  ..., -0.1067,  0.0603,  0.0472],\n",
      "        [ 0.0054, -0.0985,  0.0102,  ..., -0.0205, -0.0550, -0.0120]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Quantized INT8 model sample (first layer weight):\n",
      "tensor([[-0.0164,  0.0287, -0.0246,  ...,  0.0164,  0.0739,  0.0574],\n",
      "        [-0.0369,  0.0369, -0.0451,  ..., -0.0533,  0.1395,  0.0082],\n",
      "        [ 0.0123,  0.0328,  0.0123,  ..., -0.0246,  0.0246, -0.0451],\n",
      "        ...,\n",
      "        [-0.0082,  0.0492,  0.0533,  ...,  0.0287,  0.0533, -0.0533],\n",
      "        [-0.0164,  0.0944,  0.0574,  ..., -0.1067,  0.0615,  0.0492],\n",
      "        [ 0.0041, -0.0985,  0.0082,  ..., -0.0205, -0.0533, -0.0123]],\n",
      "       size=(768, 768), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.00410281540825963,\n",
      "       zero_point=0)\n",
      "\n",
      "--- INT8 Model State Dict for 8L Bert-Base Saved to: ./models/bert_pruned_8L_int8_ptq.pt ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import os\n",
    "\n",
    "# --- 1. 配置 ---\n",
    "# 输入模型：我们剪枝并微调好的8层FP32 Bert-Base模型\n",
    "PRUNED_FP32_MODEL_PATH = \"./models/bert_pruned_8_layers_finetuned/best_model\"\n",
    "\n",
    "# 输出文件：保存INT8量化后模型权重的文件路径\n",
    "QUANTIZED_INT8_SAVE_PATH = \"./models/bert_pruned_8L_int8_ptq.pt\"\n",
    "\n",
    "print(f\"Input model (FP32): {PRUNED_FP32_MODEL_PATH}\")\n",
    "print(f\"Output state_dict file for INT8 model: {QUANTIZED_INT8_SAVE_PATH}\")\n",
    "\n",
    "# --- 2. 加载 FP32 模型 ---\n",
    "# 加载到 CPU 上，因为量化是为 CPU 准备的\n",
    "model_fp32 = AutoModelForSequenceClassification.from_pretrained(PRUNED_FP32_MODEL_PATH)\n",
    "model_fp32.to(\"cpu\")\n",
    "model_fp32.eval()\n",
    "\n",
    "# --- 3. 应用动态量化 ---\n",
    "# 导入 PyTorch 的量化模块\n",
    "import torch.quantization\n",
    "\n",
    "# 对模型中的所有线性层 (torch.nn.Linear) 应用动态量化\n",
    "# 这是 Transformer 模型中主要的计算密集型部分\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,\n",
    "    {torch.nn.Linear}, # 指定要量化的层类型\n",
    "    dtype=torch.qint8  # 指定目标数据类型为 int8\n",
    ")\n",
    "\n",
    "print(\"\\nModel successfully quantized to INT8.\")\n",
    "print(\"Original FP32 model sample (first layer weight):\")\n",
    "print(model_fp32.bert.encoder.layer[0].attention.self.query.weight)\n",
    "print(\"\\nQuantized INT8 model sample (first layer weight):\")\n",
    "print(model_int8.bert.encoder.layer[0].attention.self.query.weight()) # 注意 quantized 模块的访问方式\n",
    "\n",
    "# --- 4. 保存量化后的模型权重 ---\n",
    "# 确保目录存在\n",
    "os.makedirs(os.path.dirname(QUANTIZED_INT8_SAVE_PATH), exist_ok=True)\n",
    "\n",
    "# 我们只保存模型的 \"state_dict\" (权重和参数)\n",
    "torch.save(model_int8.state_dict(), QUANTIZED_INT8_SAVE_PATH)\n",
    "\n",
    "print(f\"\\n--- INT8 Model State Dict for 8L Bert-Base Saved to: {QUANTIZED_INT8_SAVE_PATH} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32106700-53af-4cfd-aa28-431ae889c17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
